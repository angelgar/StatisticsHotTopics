---
title: "Final Report"
author: "Hyewon Choi, Angel Garcia de la Garza, Soo Hyun Kim, Rebecca Venetianer"
output:
  html_document:
    code_folding: hide
  pdf_document: default
---

```{r setup, include = FALSE}

# We load the required libraries

library(flexdashboard)
library(plyr)
library(ggplot2)
library(ggridges)
library(plotly)
library(dplyr)
library(readr)
library(stringr)
library(tidyverse)
library(janitor)
library(wordcloud)
library(RColorBrewer)
library(knitr)
library(Hmisc)
library(forcats)
library(tidytext)
```

### Data access

The project is located inside our GitHub <a href="https://github.com/angelgar/StatisticsHotTopics">repository</a>. Inside this repository includes an "analysis" folder, which further includes our <a href="https://github.com/angelgar/StatisticsHotTopics/tree/master/analysis/data">data</a> folder. The final dataset is named "jasa_articles_dataset_complete" and can be found <a href="https://github.com/angelgar/StatisticsHotTopics/blob/master/analysis/data/jasa_articles_dataset_complete.csv">here</a>.

## Motivation

The goal of our analysis is to understand the evolution of statistics over time, specifically in regards to "hot topics" used in research. We analyzed the titles of articles published in the Journal of the American Statistical Association (JASA) since 1888, as well as other factors such as authors, number of citations, etc. We used data gathered to create exploratory plots regarding the aforementioned factors. Specifically, we then found correlations of specific "hot topics" with article impact as measured by citations and collaboration.

## Related work

When beginning this project, we struggled to come to a conclusion about the type of data we wanted to analyze and represent visually. We agreed on the importance of creating a comprehensive visual for readers to explore, but had a plethora of ideas about the topic. After exploring datasets on happiness, crime rates, and general health data from NHANES, we switched gears and decided to create our own dataset using published papers on JASA from 1888 to today. Each group member comes with various interests in health, data analytics, data visualization, etc. Our combined interest in statistics makes the exploration of statistical hot topics over time a perfect fit.

## Initial questions

The main question of interest is to understand if/how keywords used in the titles of JASA-published papers have evolved over time, and when certain topics arose to be of interest in research. While scraping the data and further investigating the JASA website, we came up with other questions, such as how do certain keywords affect the number of citations, and how has the average number of authors changed over time, and what is the most common word in titles for each year?

## Data

We scraped this data from the Journal of the American Statistical Association website. We used the "rvest" package to download it from the internet and the the "SelectorGadget" tool to find the correct CSS selectors. We first obtained the issue number, chapter and publication year. With this information, we were able to access the issue-specific site and scraped the tile, number of views, and number of citations for each of these articles. Furthermore, we accessed the individual article's website to scrape the list of authors for each article. 

This code is used to scrape the year and volume from the JASA website:

```{r year_volume_scrape, eval = FALSE}

# This code is used to scrape the year and volume from the JASA website

url <- "http://www.tandfonline.com/loi/uasa20"

h = read_html(url)

volume_year = h %>%
    html_nodes(".volume_link h3") %>%
    html_text()

list_volume_year = str_split(volume_year, pattern = " ") 

# I did some data reshuffling to create the clean dataset

data_volume_year = matrix(data = unlist(list_volume_year), nrow = 75, ncol = 112) %>%
                    t() %>%
                    .[, which(.[1,] == "Volume"):(which(.[1,] == "Volume") + 2)] %>%
                    as.data.frame() %>%
                    as.tbl() %>%
                    separate(V3, into = c("Year", "remove"), sep = "\n") %>%
                    separate(Year, into = c("Year", "remove2"), sep = "-") %>%
                    mutate(Year = pmax(Year, remove2, na.rm = T),
                           Volume = as.character(V2)) %>%
                    select(-remove, -remove2, -V1, -V2)
  
```

This code uses the year and volume to extract the issue numbers for each year and volume:

```{r issue_volume_scrap, eval = FALSE}

# After having year and volume we scrape the issue

read_issues <- function(issue.year, issue.volume) {
  
  url <- str_c("http://www.tandfonline.com/loi/uasa20?open=",
               issue.volume,"&year=",issue.year,"&repitition")

  h = read_html(url)
  
  issue_list = h %>%
    html_nodes(".issue-num") %>%
    html_text() %>% str_split(pattern = "Issue ") %>%
    unlist() %>%
    .[which(.  != "")]
  
  data_frame(Year = rep(issue.year, length(issue_list)), 
             Volume = rep(issue.volume, length(issue_list)), 
             Issue = issue_list)
             
}

# Use map to create a dataset for each of year and volume

issues_volume <- map2_df(data_volume_year$Year, data_volume_year$Volume, ~read_issues(.x,.y))

```

This is the function used to get the article specific data for each issue:

```{r issue_articles_func, eval = FALSE}

# This function goes through each issue and pulls all data for each article.

read_articles <- function(volume, issue) {
  
  url <- str_c("http://www.tandfonline.com/toc/uasa20/",volume,"/",issue,"?nav=tocList")
  
  print(url)
  
  h = read_html(url)
  
  ## Reads in html and pulls specific CSS selectors
  
  title = h %>%
    html_nodes(".hlFld-Title") %>%
    html_text() %>% 
    as.data.frame(stringsAsFactors = FALSE) %>%
    rename(title = ".")
  
  url_data = h %>%
    html_nodes(".tocDeliverFormatsLinks > a:nth-child(1)") %>%
    html_attr('href') %>%
    str_c("http://www.tandfonline.com",.) %>%
    as.data.frame(stringsAsFactors = FALSE) %>%
    rename(urls = ".")
  
  views = h %>%
    html_nodes("li:nth-child(1) span") %>%
    html_text() %>% 
    as.data.frame(stringsAsFactors = FALSE) %>% 
    rename(views = ".")
  
  citations = h %>%
    html_nodes("li:nth-child(2) span") %>%
    html_text() %>% 
    as.data.frame(stringsAsFactors = FALSE) %>% 
    rename(citations = ".")
  
  # I needed to scrape the authors from the article specific site
  # This function does that
  
  scrap_authors <- function(url_author) {
    
    h_paper = read_html(url_author)
    citation_link = h_paper %>%
      html_nodes(".downloadCitations a") %>%
      html_attr('href') %>%
      str_c("http://www.tandfonline.com",.)
    
    h_citation = read_html(citation_link)
    
    authors = h_citation %>%
      html_nodes(".entryAuthor") %>%
      html_text() %>%
      as.data.frame(stringsAsFactors = FALSE) %>%
      rename(authors = ".")
    
  }
  
  # I have to use map to go through each article in an issue
  
  authors <- map_df(url_data$urls, ~scrap_authors(.x))
  
  
  # Clean the dataset
  
  title_views_citations = cbind(Issue = as.character(rep(issue, dim(title)[1])), 
                                Volume = rep(volume, dim(title)[1]),
                                title,
                                authors,
                                views,
                                citations) %>% 
                                as_tibble(stringsAsFactors = FALSE) %>%
                                mutate(Issue = as.character(Issue))
  
}



```

It takes a long time to run the above function. We broke it down into several calls:

```{r calls_scrap, eval = FALSE}

## It takes hours to run each call. We broke this into several calls

issues_volume <- read_csv("./analysis/data/data_issues_volume.csv")

## First 20 issues
start.time <- Sys.time()
articles_data_1_20 <- map2_df(issues_volume$Volume[1:20], 
                              issues_volume$Issue[1:20], ~read_articles(.x,.y))
write_csv(articles_data_1_20, "./analysis/data/output_issues_1_20.csv")
time.taken <- start.time - Sys.time()
time.taken

## From 21 to 70 issue
start.time <- Sys.time()
articles_data_21_70 <- map2_df(issues_volume$Volume[21:70], 
                               issues_volume$Issue[21:70], ~read_articles(.x,.y))
write_csv(articles_data_21_70, "./analysis/data/output_issues_21_70.csv")
time.taken <- start.time - Sys.time()
time.taken

## From 71 to 170 issue
## For Soohyun

start.time <- Sys.time()
articles_data_71_170 <- map2_df(issues_volume$Volume[71:170], 
                                issues_volume$Issue[71:170], ~read_articles(.x,.y))
write_csv(articles_data_71_170, "./analysis/data/output_issues_71_170.csv")
time.taken <- start.time - Sys.time()
time.taken

## From 171 to 270 issue

start.time <- Sys.time()
articles_data_171_270 <- map2_df(issues_volume$Volume[171:270], 
                                 issues_volume$Issue[171:270], ~read_articles(.x,.y))
write_csv(articles_data_171_270, "./analysis/data/output_issues_171_270.csv")
time.taken <- start.time - Sys.time()
time.taken

## From 271 to 370 issue
start.time <- Sys.time()
articles_data_271_370 <- map2_df(issues_volume$Volume[271:370], 
                                 issues_volume$Issue[271:370], ~read_articles(.x,.y))
write_csv(articles_data_271_370, "./analysis/data/output_issues_271_370.csv")
time.taken <- start.time - Sys.time()
time.taken

## Remove supplement in 
issues_volume <- issues_volume %>%
                  filter(Issue != "Supp 1")

## From 371 to 520 issue
start.time <- Sys.time()
articles_data_371_520 <- map2_df(issues_volume$Volume[371:520], 
                                 issues_volume$Issue[371:520], ~read_articles(.x,.y))
write_csv(articles_data_371_520, "./analysis/data/output_issues_371_520.csv")
time.taken <- start.time - Sys.time()
time.taken


```

I merge all the output together here:

```{r merge_data, eval = FALSE}

# This sample merges all previous samples and creates an overall dataset

issues_volume <- read_csv("./analysis/data/data_issues_volume.csv", 
                          col_types = "ccc")
issues_1_20 <- read_csv("./analysis/data/output_issues_1_20.csv", 
                          col_types = "cccccc")
issues_21_70 <- read_csv("./analysis/data/output_issues_21_70.csv", 
                          col_types = "cccccc")
issues_71_170 <- read_csv("./analysis/data/output_issues_71_170.csv", 
                          col_types = "cccccc")
issues_171_270 <- read_csv("./analysis/data/output_issues_171_270.csv", 
                          col_types = "cccccc")
issues_271_370 <- read_csv("./analysis/data/output_issues_271_370.csv", 
                          col_types = "cccccc")
issues_371_520 <- read_csv("./analysis/data/output_issues_371_520.csv", 
                          col_types = "cccccc")

# Merge datasets using rbind

data_articles <- rbind(issues_1_20,
                       issues_21_70,
                       issues_71_170,
                       issues_171_270,
                       issues_271_370,
                       issues_371_520)

## Use left_join to merge datasets
## Get rid of articles that aren't actually articles
## Accents make my computer crash so I got rid of any special characters

data_articles <- left_join(data_articles, issues_volume, by = c("Issue","Volume")) %>%
  filter(title != "Comment",
         title != "Book Review",
         title != "Correction",
         !grepl("Comment",title),
         title != "Rejoinder",
         title != "A Rejoinder",
         title != "Editorial",
         title != "Editors Report",
         title != "Reply",
         complete.cases(.)) %>%
  mutate(title = stringi::stri_trans_general(title, "latin-ascii"),
         authors = stringi::stri_trans_general(authors, "latin-ascii"))

write_csv(data_articles, "./analysis/data/jasa_articles_dataset_complete.csv")

```

We started the data cleaning process by eliminating those entries in our dataset that were not statistical articles. These included all those entries in the "title" variable that contained words such as "Comment", "Book Review", "Correction", and "Editorial". 

We proceeded to generate two new dataframes for the purpose of convenience when creating plots. In the dataframe "jasa_articles", we split the author column so that entries of author_1 to author_19 columns contain only one author. We also added an id column, in order to make identifying articles convenient from here on forward. 

```{r jasa_articles_df, eval = TRUE, warning = FALSE}

# We load our dataset containing the articles

jasa_articles = readr::read_csv("./data/jasa_articles_dataset_complete.csv",
                                col_types = "ccccccc") 

# Separate authors into invidiual columns

jasa_articles = jasa_articles %>% 
                  clean_names() %>% 
                  separate(authors, into = c("author_1", "author_19"), sep = " & ") %>%
                  separate(author_1, into = paste("author", 1:18, sep = "_"), sep = ", ") %>% 
                  mutate(id = as.character(c(1:nrow(jasa_articles)))) %>% 
                  select(id,issue:year)

# Separate words into rows
# Anti Join with stop words

data("stop_words")
jasa_rm_stopwords = jasa_articles %>%
                      unnest_tokens(word, title) %>%
                      anti_join(stop_words, by = "word")

```

For the dataframe, "jasa_rm_stopwords", we unnest the "title" column; the "word" column now contains each of the words from its respective title. Also, note that we kept all hyphenated words as one word (we wanted to keep the word "p-value" as one). We also joined jasa_articles and jasa_rm_stopwords so that we have a column for each unnested word and its corresponding title column. After some analysis, some words (for example, prepositions, conjunctions, articles, etc.) were found to be most frequent by year, but were uninformative, so we removed them. 

```{r jasa_rm_stopwords_df, eval = FALSE}

## We remove other words that are meaningless for the analysis.

jasa_rm_stopwords = jasa_rm_stopwords %>% 
  filter(word %in% c("A", "and", "a", "an", "An", "1958.", "at", "the", 
                     "The", "of", "in", "by", "for", "the", "with", "With", 
                     "by", "or", "on", "On", "Pp.", "because", "under", 
                     "about", "over", "to", "New", "Discussion", "Marriage", 
                     "1929", "Index", "Insurance", "Recent", "States", "is", 
                     "Two", "Massachusetts", "Hampshire", "Note", "Time", 
                     "When", "when", "York:", "Some", "1967", "from", "United", 
                     "Vital", "Book", "Note", "Some", "Recent", "Volume")) 

```

Lastly, we chose some words (contained in the "words" vector) manually that we wanted to analyze. We chose some words that did not necessarily appear most frequently for each year, but were more interesting to us. It should be noted that "Bayes" would be matched to "Bayes" and "Bayesian", "estimate" would be matched to "estimate" and "estimate", "parametric" would be matched to "non-parametric", "parametric" and "semi-parametric" and "sampl" would be matched to "sample" and "sampling". We created a new column in "jasa_rm_stopwords" that indicated what the words were matched to, because we would like to group similar words together, as explained above. The final dataframe was written as a csv file and saved into our data folder.

```{r keywords_data, eval = FALSE}

# These are our keywords

words = c("analysis", "bayes", "bootstrap", "clinical", "data", 
          "estimat", "inference", "model", "parametric", "p-value", 
          "regression", "sampl", "statistic")


# We loop through each keyword to find the matches
# We only save the specific keyword for that match.

jasa_rm_stopwords$matched_word <- rep(NA, dim(jasa_rm_stopwords)[1])

for (i in words) {
  jasa_rm_stopwords$matched_word[grep(i, 
                                      jasa_rm_stopwords$word, 
                                      ignore.case = T)] <- i
}

write_csv(jasa_rm_stopwords, "./data/jasa_rm_stopwords.csv")

```

## Exploratory analysis

After data were cleaned, we created visualization plots to illustrate our questions of interest. Once data were in a readable format for plot creation, minimal steps were needed between data cleaning and plot creation. Early on in the evolution of our project, we determined the types of questions we wanted to ask, and how to appropriately answer these questions visually. We concluded on representing our output through the use of four plots, in addition to one on the landing page of our website.

The landing page of our website is a word cloud to illustrate the frequency of the top 100 words used, with larger words indicating greater frequency. This plot serves as an introduction/opening to the additional plots presented on a flexdashboard. 

```{r word_cloud, message = FALSE, warning = FALSE, out.width = '250%'}


jasa_rm_stopwords <- read_csv("./data/jasa_rm_stopwords.csv", 
                              col_types = "dccccccccccccccccccccccccdcc")

# We use our word dataset to find a wordcloud of most used words

word_map_data <- jasa_rm_stopwords %>% 
                    dplyr::count(word, sort = TRUE) %>%
                    top_n(200)

wordcloud(words = word_map_data$word, freq = word_map_data$n, 
          min.freq = 50, max.words = 100, random.order = FALSE, 
          rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
```

#### The additional plots include: 

#### (1) Top word per year

For the first plot, we created a barplot with "year" on the x-axis and "proportion" on the y-axis. Each bar in the plot indicates the percentage of article titles the “top word” of the year appears in, for that year. For this plot, we removed stop words and other non-useful words from being the most common, allowing only relevant words such as "model" to appear as the most frequent. Without doing so, the plot would produce data showing words such as "the" and "and" as the most frequent.

```{r plot_1 , message = FALSE, warning = FALSE}

## Proportion of top words by year

jasa_rm_stopwords = read_csv("./data/jasa_rm_stopwords.csv")
 
pop_word_prop = jasa_rm_stopwords %>%
                  group_by(year) %>%
                  dplyr::count(word) 
  
pop_word_prop = full_join(pop_word_prop, jasa_rm_stopwords) %>% 
                  group_by(year) %>% 
                  mutate(proportion = n/length(unique(title))) %>%
                  arrange(proportion) %>%
                  top_n(1) %>%
                  ggplot(aes(x = year, y = proportion, fill = word)) +
                    geom_bar(stat = "identity", position = "dodge") +
                    labs(title = "Top Words by Year")
 
ggplotly(pop_word_prop)
 
# "statistics" in late 1800's to early 1900's popular
# "data", "models" popular in late 1900's and 2000's ("data" in 2006)

```

#### (2) Trend of keywords over time

For the second plot, we focused on 13 keywords and created a ridge plot with "year" on the x-axis and and keyword frequency on the y-axis. These 13 keywords were chosen as words that all group members thought would be important to examine the trends of. This plot allowed us to visually demonstrate increasing and decreasing trends of various words over time. "Proportion" in this plot corresponds to the same ratio method used for the previous plot. 

```{r plot_2, message = FALSE, warning = FALSE}

# Trends of statistical "hot topics" from 1888 - 2017
# We plot the distribution of proportion 
# Proportion is how articles the word has over total number of articles per year

words_ridges = jasa_rm_stopwords %>%
  filter(matched_word != "NA") %>%
  ggplot(aes(x = year, y = matched_word, fill = matched_word)) +
  geom_density_ridges() +
  labs(title = "Trends of Hot Topics from 1888 to 2017") + 
  theme(legend.position = "none") +
  ylab("Keyword") +
  xlab("Year")

words_ridges

```

#### (3) Number of collaborators per article per year

For the third plot, we created a scatterplot with "year" on the x-axis and "mean number of authors" on the y-axis, with "mean number of authors" referring to the mean number of collaborators per article per year. The goal of this was to understand trends in collaboration over time, with the hypothesis that collaboration has increased, and therefore the mean number of collaborators per article per year has also increased.

```{r plo_3, message = FALSE, warning = FALSE}

# Number of collaborators per article for every year from 1888 - 2017
# We calculate the mean number of collaborators for each year

jasa_articles <- read_csv(".//data/jasa_articles_dataset_complete.csv")
 
jasa_articles <- jasa_articles %>%
  clean_names() %>%
  separate(authors, into = c("author1", "author19"), sep = " & ") %>%
  separate(author1, into = paste("author", 1:18), sep = ", ") %>%
  unnest_tokens(word, title)
data("stop_words")
 
jasa_articles = anti_join(jasa_articles, stop_words)
 
jasa_articles <- jasa_articles %>%
  dplyr::select(`author 1`:author19, year)
 
no_authors_year <- jasa_articles %>%
  dplyr::mutate( number_authors = rowSums(!is.na(jasa_articles)) - 1) %>%
  dplyr::select(number_authors, year) %>%
  group_by(year) %>%
  dplyr::summarize( Mean_no_authors = mean(number_authors))
 
finalplot <- no_authors_year %>%
  ggplot(aes(x = year, y = Mean_no_authors, color = Mean_no_authors)) +
  geom_point() +
  geom_smooth(se = FALSE, color = "dodgerblue3") + labs(
    title = "Trend of the mean number of authors",
    x = "Year",
    y = "Mean number of authors"
    ) +
  theme(legend.position = "none")

ggplotly(finalplot)

```

#### (4) Number of citations per keyword. 

Similarly to the third plot, we created the fourth plot to determine a visual relationship between "number of citations" and "keyword". Number of citations refers to the number of people who have cited the given paper directly from JASA. Keywords were selected by group members to determine "hot topics" of interest. We did not have any strong hypotheses regarding the appearance of this plot, but it is interesting to understand the large differences in citations based on certain keywords.

```{r plot_4, message = FALSE, warning = FALSE}

# Number of total citations per statistical "hot topic" 

jasa_rm_sw = read_csv(".//data/jasa_rm_stopwords.csv")
 
citation_top10 <- jasa_rm_sw %>%
  dplyr::filter(!is.na(matched_word)) %>%
  dplyr::select(id, citations, matched_word) %>%
  group_by(matched_word) %>%
  dplyr::summarize( Total_number_of_citations = sum(citations))  %>%
  rename(word = matched_word)


citation_top10 %>% 
  dplyr::mutate(Hot_topics = fct_reorder(word, Total_number_of_citations)) %>% 
  plot_ly(x = ~Hot_topics, y = ~Total_number_of_citations, color = ~Hot_topics, type = "bar") %>%
  layout(
    xaxis = list(
      type = 'category',
      title = 'Keyword'
    ),
    yaxis = list(
      title = 'Total Number of Citations'
    )
  )

```

## Discussion

Since our analysis relied on visual representations of data scraped form JASA, no formal statistical conclusions can be made. However, it is interesting to examine trends that appeared as a result of looking at the relationship between statistical "hot topics." Major trends are noted in the remainder of the discussion. From the first plot, we can see that "statistics" and "statistical" dominated word proportion until the 1960s, when words such as "estimation", "models", and "data" began to appear. It is important to note here that proportions of key words likely decreased over time because as more papers were published into the 1940s, there is more variability in the titles. From the second plot, we can see that only "statistic" has remained relatively steady over time, and there was a spike in "p-value" after the year 2000, consistent with the growing attention p-values are getting. The third plot shows a clear positive trend in the number of authors in each article, illustrating an increase in collaboration on statistical papers. It will be interesting to note if this trend continues to increase and/or ever reaches a plateau or decrease in the trend. From the fourth plot we know that "estimate", "model", and "regression" generate the greatest number of citations and "p-value", "clinical", and "bootstrap" generate the fewest number of citations.

For future analysis, it would be interesting to examine the relationship between tagged phrases by authors and number of citations, trends in the words over time, etc. Additionally, it would be useful to follow the project with a formal statistical analysis of when certain words increased in prevalence, not only in titles but in words used throughout the papers.


